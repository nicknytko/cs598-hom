%%
% Please see https://bitbucket.org/rivanvx/beamer/wiki/Home for obtaining beamer.
%%
\documentclass{beamer}
\usetheme{Warsaw}

\usepackage{blkarray}
\usepackage{bigstrut}
\usepackage{amsmath}

\title{Higher Order Markov Chains and Spacey Walks}
\author{Nicolas Nytko}
\date{\today}

\begin{document}
\frame{\titlepage}

% Review of first-order markov chains
\begin{frame}
\frametitle{Markov Chains}
{\bf Markov Chains} are stochastic models describing a sequence of states with a probability of transitioning between each.
\begin{itemize}
\item Defined as matrix: ${\bf P} \in \mathbb{R}^{n\times n}$, ${\bf P}_{ij}$ gives probability of moving to state $i$ from $j$, for chain with $n$ states.
\item Given distribution/state vector ${\bf x}^{(n)}$, one step of a random walk is represented by matrix-vector product ${\bf x}^{(n+1)} = {\bf P} {\bf x}^{(n)} \enskip \Leftrightarrow \enskip x^{(n+1)}_i = \sum_j P_{ij} x^{(n)}_j $.
\end{itemize}

\begin{block}{}
{\bf Time homogeneous}: each transition probability is independent of the time step $t$.
\end{block}

\begin{block}{}
{\bf Irreducible}: every state can be reached from every other state.
\end{block}

\end{frame}

\begin{frame}
\frametitle{Canonical Examples}

\textbf{Google PageRank}
\begin{itemize}
\item Markov chain represents the ``idealized'' web surfer.  States are webpages and transition probability is given by number of outgoing links between pages.
\item Ranking is determined by probability of being on certain page.
\end{itemize}

\textbf{Board Games}
\begin{itemize}
\item Each spot on game board is a state.
\item Probability of moving between spots is given by dice, spinner, etc.
\end{itemize}

\textbf{Weather}
\begin{itemize}
\item States are possible weather conditions.
\item Tomorrow's weather is randomly chosen based on today's conditions.
\end{itemize}

\end{frame}


% Introduction of steady-state distribution
\begin{frame}
\frametitle{Steady-State Distribution}

\begin{block}{}
\textbf{Steady state} vector $\boldsymbol{\pi} \in \mathbb{R}^n$ does not change when multiplied by Markov chain:
\[ \boldsymbol{\pi} = {\bf P} \boldsymbol{\pi} \]
\end{block}

\begin{itemize}
\item Average distribution as random walk tends to length infinity, $\lim_{k\to\infty} {\bf P}^k{\bf x}$
\item Is equivalent to \textbf{Perron vector} of matrix, dominant eigenvector with nonnegative components.
\item Compute iteratively with power method: ${\bf x}^{(k+1) } = {\bf Px}^{(k)}$.
\end{itemize}

\begin{block}{}
Unique steady states are guaranteed for \textbf{irreducible} Markov chains.
\end{block}
\end{frame}

% Introduce higher order chains as matricised tensors
\begin{frame}
\frametitle{Markov Chains with Memory}
What if we want our chain to \textbf{remember} where it has been?  Introduce \textbf{$\boldsymbol{m}$-order Markov chain}, where transition probability depends on previous $m$ states.  
\begin{itemize}
\item Probability is now stored as ${\bf P} \in \mathbb{R}^{n \times n^m}$, where columns are a $m$-tuple of each state permutation.
\item Random step becomes slightly more complicated than just a \textit{mat-vec}, will see a better way of doing this.
\end{itemize}

%\[ {\bf P} = 
%\begin{blockarray}{ccc}
% & s_0 & s_1 \\
%\begin{block}{c[cc]}
%  \bigstrut[t] 
%  \left(s_0, s_0\right) & p_{0,0} & p_{0,1} \\
%  \left(s_0, s_1\right) & p_{1,0} & p_{1,1} \\
%  \left(s_1, s_0\right) & p_{2,0} & p_{2,1} \\
%  \left(s_1, s_1\right) & p_{3,0} & p_{3,1}
%  \bigstrut[b]
%\end{block}
%\end{blockarray} \]

\begin{block}{}
\[{\bf P} = 
\begin{blockarray}{ccccc}
	& \left(s_0, s_0\right) & \left(s_0, s_1\right) & \left(s_1, s_0\right) & \left(s_1, s_1\right) \\
	\begin{block}{c[cccc]}
		\bigstrut[t]
		s_0 & p_{1,1} & p_{1,2} & p_{1,3} & p_{1,4} \\
		s_1 & p_{2,1} & p_{2,2} & p_{2,3} & p_{2,4}
		\bigstrut[b]
	\end{block}
\end{blockarray}\]

\begin{center}
\begin{small}
(Example indexing for $2^{\text{nd}}$ order chain with $n=2$ states.)
\end{small}
\end{center}

\end{block}

\end{frame}

% Introduce higher order chain
\begin{frame}
\frametitle{Higher Order Markov Chains}

We can \textbf{fold the last dimension} of the higher-order Markov matrix to obtain a higher dimensional \textbf{tensor}.

\begin{block}{}
Order $m$ Markov chain with $n$ states is order $m+1$ tensor 

\[\mathcal{P} \in \mathbb{R}^{\overbrace{n\times n\times \cdots \times n}^{m+1\text{ dimensions}}}\]

Transition probability indexed by $P_{i_{(n+1)},i_{(n)},i_{(n-1)},\ldots,i_{(n-m+1)}}$
\end{block}

\begin{itemize}
\item State/distribution is stored as order $m$ tensor.
\item Example random step for order 2 Markov chain: $X^{(n+1)}_{ij} = \sum_k P_{ijk} X^{(n+1)}_{jk}$
\item Tensor form gives nice analysis properties --- will see shortly.
\end{itemize}
	
\end{frame}

\begin{frame}
\frametitle{Higher Order Steady-State Distribution}

\begin{block}{}
\textbf{Steady state} tensor ${\bf X} \in \mathbb{R}^{n \times n}$ does not change when multiplied along first mode.
\[ X_{ij} = \sum_k P_{ijk} X_{jk} \]
\end{block}
\begin{itemize}	
\item Conceptually simple, but requires $\mathcal{O}\left(n^m\right)$ space to store state tensor.
\end{itemize}
\begin{block}{}
Replace state tensor with \textbf{low rank approximation} ${\bf x} \in \mathbb{R}^n$:
\[ x_i = \sum_{jk} P_{ijk} x_j x_k \]
Actually just the \textbf{$\boldsymbol{z}$-eigenvector} of $\mathcal{P}$!
\end{block}

\end{frame}



\begin{frame}	
\frametitle{Spacey Walks}
\end{frame}



\begin{frame}
\frametitle{Applications, Examples}
\end{frame}

\begin{frame}
\frametitle{References}
\nocite*{}
\bibliographystyle{plain}
\bibliography{ho-markov}
\end{frame}

\end{document}